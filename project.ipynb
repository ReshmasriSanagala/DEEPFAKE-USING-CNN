{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nfTe4QoFNP-",
        "outputId": "44e4778f-2cc9-4d08-9c04-eb426dba1a69",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (4.9.0.80)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow opencv-python opencv-python-headless numpy scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install opencv-python-headless\n",
        "!pip install matplotlib\n",
        "!pip install albumentations\n",
        "!pip install tqdm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lyt9goW4GcUt",
        "outputId": "3b63d24f-c8b6-428f-fae4-d06ba103112a",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.0+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (4.9.0.80)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python-headless) (1.25.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.3.1)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.11.4)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.19.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.1)\n",
            "Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.0.4)\n",
            "Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.9.0.80)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from qudida>=0.0.4->albumentations) (1.2.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from qudida>=0.0.4->albumentations) (4.11.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (3.3)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (9.4.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (2024.5.22)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (1.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (24.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.5.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBzxVuKutXzS",
        "outputId": "1b3ed019-0e61-4a54-d25c-d4b364d98a5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcbttYbuttqD",
        "outputId": "811b8c0e-6cca-42d2-9cf9-d589d40a70ec",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'10TH LONG MEMO.pdf'\n",
            "'12th bona fide .pdf'\n",
            "'12th T.C'\n",
            " 16823229568508157311691081052507.jpg\n",
            " 2.pdf\n",
            " AADHAR.pdf\n",
            "'ALLOTMENT ORDER.pdf'\n",
            "'Bank challan.pdf'\n",
            "'Bonafide 11-12.pdf'\n",
            "'BONAFIDE 6 TO 10.pdf'\n",
            "'BTech 1st sem_ S.Reshma sri_ cse c _21881A05J1_unit2&3_.pdf'\n",
            "'chem sample paper 2 gayatri.pdf'\n",
            " code.mp4\n",
            "'Colab Notebooks'\n",
            "'Contact Information.gform'\n",
            "'Contact Information (Responses).gsheet'\n",
            "'Copy of RESUME (1).gsite'\n",
            "'Copy of RESUME.gsite'\n",
            "'css code.txt'\n",
            "'Edited - S.JAHNAVI 3747 X JASMINE  TEL (1).pdf'\n",
            "'Edited - S.JAHNAVI 3747 X JASMINE  TEL.pdf'\n",
            "'EPASS ACKNOWLEDGEMENT.pdf'\n",
            "'EPASS APPLICATION.pdf'\n",
            "'e-P A S S.pdf'\n",
            " fake\n",
            "\"'FAMILY FINANCIAL STATEMENT' .pdf\"\n",
            "'HALL TICKET TS EAMCET.pdf'\n",
            "\"I am sharing 'photo' with you\"\n",
            "'IEE CARD.png'\n",
            "'INTER SHORT MEMO.pdf'\n",
            "'JEWE RANK CARD.pdf'\n",
            "'jimmy fallon-fake.mp4'\n",
            "'JOINING ORDER.pdf'\n",
            "'key 2.pdf'\n",
            "'key 3 gayatri.pdf'\n",
            "'Luxorious and alluring refreshment farm house at best rental price on best platform.pdf'\n",
            " passport.pdf\n",
            "'Photo album.gslides'\n",
            " photo.PDF\n",
            "'Portfolio (1).gsite'\n",
            " portfolio.gsite\n",
            " Portfolio.gsite\n",
            " projectpics\n",
            " real\n",
            "'RENTERA INTERNSHIP '\n",
            "'rentera payment.jpeg'\n",
            " RESUME.gsite\n",
            " Screenshot_20240318_160838_PhonePe.jpg\n",
            "\"'Share ur experience '  response\"\n",
            " Sign.pdf\n",
            " SRS.jee\n",
            "'SRS JEE'\n",
            "'SRS VACCICENE.pdf'\n",
            "'TS EAMCET RANK CARD.pdf'\n",
            "'Untitled site (1).gsite'\n",
            "'Untitled site.gsite'\n",
            " videos\n",
            "'WhatsApp Video 2024-05-10 at profile (1).mp4'\n",
            "'WhatsApp Video 2024-05-10 at profile.mp4'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace 'your_folder_name' with the actual folder name in your MyDrive directory\n",
        "!ls /content/drive/MyDrive/real/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z94z43Aexd_k",
        "outputId": "a81e0a88-6f4e-4e07-b8a2-b6c364771e35",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " maxdefault.jpg   real_00217.jpg        real_00432.jpg\t real_00649.jpg   real_00866.jpg\n",
            " real_00001.jpg   real_00218.jpg        real_00433.jpg\t real_00650.jpg   real_00867.jpg\n",
            " real_00002.jpg   real_00219.jpg        real_00434.jpg\t real_00651.jpg   real_00868.jpg\n",
            " real_00003.jpg   real_00220.jpg        real_00435.jpg\t real_00652.jpg   real_00869.jpg\n",
            " real_00004.jpg   real_00221.jpg        real_00436.jpg\t real_00653.jpg   real_00870.jpg\n",
            " real_00005.jpg   real_00222.jpg        real_00437.jpg\t real_00654.jpg   real_00871.jpg\n",
            " real_00006.jpg   real_00223.jpg        real_00438.jpg\t real_00655.jpg   real_00872.jpg\n",
            " real_00007.jpg   real_00224.jpg        real_00439.jpg\t real_00656.jpg   real_00873.jpg\n",
            " real_00008.jpg   real_00225.jpg        real_00440.jpg\t real_00657.jpg   real_00874.jpg\n",
            " real_00009.jpg   real_00226.jpg        real_00441.jpg\t real_00658.jpg   real_00875.jpg\n",
            " real_00010.jpg   real_00227.jpg        real_00442.jpg\t real_00659.jpg   real_00876.jpg\n",
            " real_00011.jpg   real_00228.jpg        real_00443.jpg\t real_00660.jpg   real_00877.jpg\n",
            " real_00012.jpg   real_00229.jpg        real_00444.jpg\t real_00661.jpg   real_00878.jpg\n",
            " real_00013.jpg   real_00230.jpg        real_00445.jpg\t real_00662.jpg   real_00879.jpg\n",
            " real_00014.jpg   real_00231.jpg        real_00446.jpg\t real_00663.jpg   real_00880.jpg\n",
            " real_00015.jpg   real_00232.jpg        real_00447.jpg\t real_00664.jpg   real_00881.jpg\n",
            " real_00016.jpg   real_00233.jpg        real_00448.jpg\t real_00665.jpg   real_00882.jpg\n",
            " real_00017.jpg   real_00234.jpg        real_00449.jpg\t real_00666.jpg   real_00883.jpg\n",
            " real_00018.jpg   real_00235.jpg        real_00450.jpg\t real_00667.jpg   real_00884.jpg\n",
            " real_00019.jpg   real_00236.jpg        real_00451.jpg\t real_00668.jpg   real_00885.jpg\n",
            " real_00020.jpg   real_00237.jpg        real_00452.jpg\t real_00669.jpg   real_00886.jpg\n",
            " real_00021.jpg   real_00238.jpg        real_00453.jpg\t real_00670.jpg   real_00887.jpg\n",
            " real_00022.jpg   real_00239.jpg        real_00454.jpg\t real_00671.jpg   real_00888.jpg\n",
            " real_00023.jpg   real_00240.jpg        real_00455.jpg\t real_00672.jpg   real_00889.jpg\n",
            " real_00024.jpg   real_00241.jpg        real_00456.jpg\t real_00673.jpg   real_00890.jpg\n",
            " real_00025.jpg   real_00242.jpg        real_00457.jpg\t real_00674.jpg   real_00891.jpg\n",
            " real_00026.jpg   real_00243.jpg        real_00458.jpg\t real_00675.jpg   real_00892.jpg\n",
            " real_00027.jpg   real_00244.jpg        real_00459.jpg\t real_00676.jpg   real_00893.jpg\n",
            " real_00028.jpg   real_00245.jpg        real_00460.jpg\t real_00677.jpg   real_00894.jpg\n",
            " real_00029.jpg   real_00246.jpg        real_00461.jpg\t real_00678.jpg   real_00895.jpg\n",
            " real_00030.jpg   real_00247.jpg        real_00462.jpg\t real_00679.jpg   real_00896.jpg\n",
            " real_00031.jpg   real_00248.jpg        real_00463.jpg\t real_00680.jpg   real_00897.jpg\n",
            " real_00032.jpg   real_00249.jpg        real_00464.jpg\t real_00681.jpg   real_00898.jpg\n",
            " real_00033.jpg   real_00250.jpg        real_00465.jpg\t real_00682.jpg   real_00899.jpg\n",
            " real_00034.jpg   real_00251.jpg        real_00466.jpg\t real_00683.jpg   real_00900.jpg\n",
            " real_00035.jpg   real_00252.jpg        real_00467.jpg\t real_00684.jpg   real_00901.jpg\n",
            " real_00036.jpg   real_00253.jpg        real_00468.jpg\t real_00685.jpg   real_00902.jpg\n",
            " real_00037.jpg   real_00254.jpg        real_00469.jpg\t real_00686.jpg   real_00903.jpg\n",
            " real_00038.jpg   real_00255.jpg        real_00470.jpg\t real_00687.jpg   real_00904.jpg\n",
            " real_00039.jpg   real_00256.jpg        real_00471.jpg\t real_00688.jpg   real_00905.jpg\n",
            " real_00040.jpg   real_00257.jpg        real_00472.jpg\t real_00689.jpg   real_00906.jpg\n",
            " real_00041.jpg   real_00258.jpg        real_00473.jpg\t real_00690.jpg   real_00907.jpg\n",
            " real_00042.jpg   real_00259.jpg        real_00474.jpg\t real_00691.jpg   real_00908.jpg\n",
            " real_00043.jpg   real_00260.jpg        real_00475.jpg\t real_00692.jpg   real_00909.jpg\n",
            " real_00044.jpg   real_00261.jpg        real_00476.jpg\t real_00693.jpg   real_00910.jpg\n",
            " real_00045.jpg   real_00262.jpg        real_00477.jpg\t real_00694.jpg   real_00911.jpg\n",
            " real_00046.jpg   real_00263.jpg        real_00478.jpg\t real_00695.jpg   real_00912.jpg\n",
            " real_00047.jpg   real_00264.jpg        real_00479.jpg\t real_00696.jpg   real_00913.jpg\n",
            " real_00048.jpg   real_00265.jpg        real_00480.jpg\t real_00697.jpg   real_00914.jpg\n",
            " real_00049.jpg   real_00266.jpg        real_00481.jpg\t real_00698.jpg   real_00915.jpg\n",
            " real_00050.jpg   real_00267.jpg        real_00482.jpg\t real_00699.jpg   real_00916.jpg\n",
            " real_00051.jpg   real_00268.jpg        real_00483.jpg\t real_00700.jpg   real_00917.jpg\n",
            " real_00052.jpg   real_00269.jpg        real_00484.jpg\t real_00701.jpg   real_00918.jpg\n",
            " real_00053.jpg   real_00270.jpg        real_00485.jpg\t real_00702.jpg   real_00919.jpg\n",
            " real_00054.jpg   real_00271.jpg        real_00486.jpg\t real_00703.jpg   real_00920.jpg\n",
            " real_00055.jpg   real_00272.jpg        real_00487.jpg\t real_00704.jpg   real_00921.jpg\n",
            " real_00056.jpg   real_00273.jpg        real_00488.jpg\t real_00705.jpg   real_00922.jpg\n",
            " real_00057.jpg   real_00274.jpg        real_00489.jpg\t real_00706.jpg   real_00923.jpg\n",
            " real_00058.jpg   real_00275.jpg        real_00490.jpg\t real_00707.jpg   real_00924.jpg\n",
            " real_00059.jpg   real_00276.jpg        real_00491.jpg\t real_00708.jpg   real_00925.jpg\n",
            " real_00060.jpg   real_00277.jpg        real_00492.jpg\t real_00709.jpg   real_00926.jpg\n",
            " real_00061.jpg   real_00278.jpg        real_00493.jpg\t real_00710.jpg   real_00927.jpg\n",
            " real_00062.jpg   real_00279.jpg        real_00494.jpg\t real_00711.jpg   real_00928.jpg\n",
            " real_00063.jpg   real_00280.jpg        real_00495.jpg\t real_00712.jpg   real_00929.jpg\n",
            " real_00064.jpg   real_00281.jpg        real_00496.jpg\t real_00713.jpg   real_00930.jpg\n",
            " real_00065.jpg   real_00282.jpg        real_00497.jpg\t real_00714.jpg   real_00931.jpg\n",
            " real_00066.jpg   real_00283.jpg        real_00498.jpg\t real_00715.jpg   real_00932.jpg\n",
            " real_00067.jpg   real_00284.jpg        real_00499.jpg\t real_00716.jpg   real_00933.jpg\n",
            " real_00068.jpg   real_00285.jpg        real_00500.jpg\t real_00717.jpg   real_00934.jpg\n",
            " real_00069.jpg   real_00286.jpg        real_00501.jpg\t real_00718.jpg   real_00935.jpg\n",
            " real_00070.jpg   real_00287.jpg        real_00502.jpg\t real_00719.jpg   real_00936.jpg\n",
            " real_00071.jpg   real_00288.jpg        real_00503.jpg\t real_00720.jpg   real_00937.jpg\n",
            " real_00072.jpg   real_00289.jpg        real_00504.jpg\t real_00721.jpg   real_00938.jpg\n",
            " real_00073.jpg   real_00290.jpg        real_00505.jpg\t real_00722.jpg   real_00939.jpg\n",
            " real_00074.jpg   real_00291.jpg        real_00506.jpg\t real_00723.jpg   real_00940.jpg\n",
            " real_00075.jpg   real_00292.jpg        real_00507.jpg\t real_00724.jpg   real_00941.jpg\n",
            " real_00076.jpg   real_00293.jpg        real_00508.jpg\t real_00725.jpg   real_00942.jpg\n",
            " real_00077.jpg   real_00294.jpg        real_00509.jpg\t real_00726.jpg   real_00943.jpg\n",
            " real_00078.jpg   real_00295.jpg        real_00510.jpg\t real_00727.jpg   real_00944.jpg\n",
            " real_00079.jpg   real_00296.jpg        real_00511.jpg\t real_00728.jpg   real_00945.jpg\n",
            " real_00080.jpg   real_00297.jpg        real_00512.jpg\t real_00729.jpg   real_00946.jpg\n",
            " real_00081.jpg   real_00298.jpg        real_00513.jpg\t real_00730.jpg   real_00947.jpg\n",
            " real_00082.jpg   real_00299.jpg        real_00514.jpg\t real_00731.jpg   real_00948.jpg\n",
            " real_00083.jpg   real_00300.jpg        real_00515.jpg\t real_00732.jpg   real_00949.jpg\n",
            " real_00084.jpg   real_00301.jpg        real_00516.jpg\t real_00733.jpg   real_00950.jpg\n",
            " real_00085.jpg   real_00302.jpg        real_00517.jpg\t real_00734.jpg   real_00951.jpg\n",
            " real_00086.jpg   real_00303.jpg        real_00518.jpg\t real_00735.jpg   real_00952.jpg\n",
            " real_00087.jpg   real_00304.jpg        real_00519.jpg\t real_00736.jpg   real_00953.jpg\n",
            " real_00088.jpg   real_00305.jpg        real_00520.jpg\t real_00737.jpg   real_00954.jpg\n",
            " real_00089.jpg   real_00306.jpg        real_00521.jpg\t real_00738.jpg   real_00955.jpg\n",
            " real_00090.jpg   real_00307.jpg        real_00522.jpg\t real_00739.jpg   real_00956.jpg\n",
            " real_00091.jpg   real_00308.jpg        real_00523.jpg\t real_00740.jpg   real_00957.jpg\n",
            " real_00092.jpg   real_00309.jpg        real_00524.jpg\t real_00741.jpg   real_00958.jpg\n",
            " real_00093.jpg   real_00310.jpg        real_00525.jpg\t real_00742.jpg   real_00959.jpg\n",
            " real_00094.jpg   real_00311.jpg        real_00526.jpg\t real_00743.jpg   real_00960.jpg\n",
            " real_00095.jpg   real_00312.jpg        real_00527.jpg\t real_00744.jpg   real_00961.jpg\n",
            " real_00096.jpg   real_00313.jpg        real_00528.jpg\t real_00745.jpg   real_00962.jpg\n",
            " real_00097.jpg   real_00314.jpg        real_00529.jpg\t real_00746.jpg   real_00963.jpg\n",
            " real_00098.jpg   real_00315.jpg        real_00530.jpg\t real_00747.jpg   real_00964.jpg\n",
            " real_00099.jpg   real_00316.jpg        real_00531.jpg\t real_00748.jpg   real_00965.jpg\n",
            " real_00100.jpg   real_00317.jpg        real_00532.jpg\t real_00749.jpg   real_00966.jpg\n",
            " real_00101.jpg   real_00318.jpg        real_00533.jpg\t real_00750.jpg   real_00967.jpg\n",
            " real_00102.jpg   real_00319.jpg        real_00534.jpg\t real_00751.jpg   real_00968.jpg\n",
            " real_00103.jpg   real_00320.jpg        real_00535.jpg\t real_00752.jpg   real_00969.jpg\n",
            " real_00104.jpg   real_00321.jpg        real_00536.jpg\t real_00753.jpg   real_00970.jpg\n",
            " real_00105.jpg   real_00322.jpg        real_00537.jpg\t real_00754.jpg   real_00971.jpg\n",
            " real_00106.jpg   real_00323.jpg        real_00538.jpg\t real_00755.jpg   real_00972.jpg\n",
            " real_00107.jpg   real_00324.jpg        real_00539.jpg\t real_00756.jpg   real_00973.jpg\n",
            " real_00108.jpg   real_00325.jpg        real_00540.jpg\t real_00757.jpg   real_00974.jpg\n",
            " real_00109.jpg   real_00326.jpg        real_00541.jpg\t real_00758.jpg   real_00975.jpg\n",
            " real_00110.jpg   real_00327.jpg        real_00542.jpg\t real_00759.jpg   real_00976.jpg\n",
            " real_00111.jpg   real_00328.jpg        real_00543.jpg\t real_00760.jpg   real_00977.jpg\n",
            " real_00112.jpg   real_00329.jpg        real_00544.jpg\t real_00761.jpg   real_00978.jpg\n",
            " real_00113.jpg   real_00330.jpg        real_00545.jpg\t real_00762.jpg   real_00979.jpg\n",
            " real_00114.jpg   real_00331.jpg        real_00546.jpg\t real_00763.jpg   real_00980.jpg\n",
            " real_00115.jpg   real_00332.jpg        real_00547.jpg\t real_00764.jpg   real_00981.jpg\n",
            " real_00116.jpg   real_00333.jpg        real_00548.jpg\t real_00765.jpg   real_00982.jpg\n",
            " real_00117.jpg   real_00334.jpg        real_00549.jpg\t real_00766.jpg   real_00983.jpg\n",
            " real_00118.jpg   real_00335.jpg        real_00550.jpg\t real_00767.jpg   real_00984.jpg\n",
            " real_00119.jpg   real_00336.jpg        real_00551.jpg\t real_00768.jpg   real_00985.jpg\n",
            " real_00120.jpg   real_00337.jpg        real_00552.jpg\t real_00769.jpg   real_00986.jpg\n",
            " real_00121.jpg   real_00338.jpg        real_00553.jpg\t real_00770.jpg   real_00987.jpg\n",
            " real_00122.jpg   real_00339.jpg        real_00554.jpg\t real_00771.jpg   real_00988.jpg\n",
            " real_00123.jpg   real_00340.jpg        real_00555.jpg\t real_00772.jpg   real_00989.jpg\n",
            " real_00124.jpg   real_00341.jpg        real_00556.jpg\t real_00773.jpg   real_00990.jpg\n",
            " real_00125.jpg   real_00342.jpg        real_00557.jpg\t real_00774.jpg   real_00991.jpg\n",
            " real_00126.jpg   real_00343.jpg        real_00558.jpg\t real_00775.jpg   real_00992.jpg\n",
            " real_00127.jpg   real_00344.jpg        real_00559.jpg\t real_00776.jpg   real_00993.jpg\n",
            " real_00128.jpg   real_00345.jpg        real_00560.jpg\t real_00777.jpg   real_00994.jpg\n",
            " real_00129.jpg   real_00346.jpg        real_00561.jpg\t real_00778.jpg   real_00995.jpg\n",
            " real_00130.jpg   real_00347.jpg        real_00562.jpg\t real_00779.jpg   real_00996.jpg\n",
            " real_00131.jpg   real_00348.jpg        real_00563.jpg\t real_00780.jpg   real_00997.jpg\n",
            " real_00132.jpg   real_00349.jpg        real_00564.jpg\t real_00781.jpg   real_00998.jpg\n",
            " real_00133.jpg   real_00350.jpg        real_00565.jpg\t real_00782.jpg   real_00999.jpg\n",
            " real_00134.jpg   real_00351.jpg        real_00566.jpg\t real_00783.jpg   real_01000.jpg\n",
            " real_00135.jpg   real_00352.jpg        real_00567.jpg\t real_00784.jpg   real_01001.jpg\n",
            " real_00136.jpg   real_00353.jpg        real_00568.jpg\t real_00785.jpg   real_01002.jpg\n",
            " real_00137.jpg   real_00354.jpg        real_00569.jpg\t real_00786.jpg   real_01003.jpg\n",
            " real_00138.jpg   real_00355.jpg        real_00570.jpg\t real_00787.jpg   real_01004.jpg\n",
            " real_00139.jpg   real_00356.jpg        real_00571.jpg\t real_00788.jpg   real_01005.jpg\n",
            " real_00140.jpg   real_00357.jpg        real_00572.jpg\t real_00789.jpg   real_01006.jpg\n",
            " real_00141.jpg   real_00358.jpg        real_00573.jpg\t real_00790.jpg   real_01007.jpg\n",
            " real_00142.jpg   real_00359.jpg        real_00574.jpg\t real_00791.jpg   real_01008.jpg\n",
            " real_00143.jpg   real_00360.jpg        real_00575.jpg\t real_00792.jpg   real_01009.jpg\n",
            " real_00144.jpg  'real_00361 (1).jpg'   real_00576.jpg\t real_00793.jpg   real_01010.jpg\n",
            " real_00145.jpg   real_00361.jpg        real_00577.jpg\t real_00794.jpg   real_01011.jpg\n",
            " real_00146.jpg  'real_00362 (1).jpg'   real_00578.jpg\t real_00795.jpg   real_01012.jpg\n",
            " real_00147.jpg   real_00362.jpg        real_00579.jpg\t real_00796.jpg   real_01013.jpg\n",
            " real_00148.jpg   real_00363.jpg        real_00580.jpg\t real_00797.jpg   real_01014.jpg\n",
            " real_00149.jpg   real_00364.jpg        real_00581.jpg\t real_00798.jpg   real_01015.jpg\n",
            " real_00150.jpg   real_00365.jpg        real_00582.jpg\t real_00799.jpg   real_01016.jpg\n",
            " real_00151.jpg   real_00366.jpg        real_00583.jpg\t real_00800.jpg   real_01017.jpg\n",
            " real_00152.jpg   real_00367.jpg        real_00584.jpg\t real_00801.jpg   real_01018.jpg\n",
            " real_00153.jpg   real_00368.jpg        real_00585.jpg\t real_00802.jpg   real_01019.jpg\n",
            " real_00154.jpg   real_00369.jpg        real_00586.jpg\t real_00803.jpg   real_01020.jpg\n",
            " real_00155.jpg   real_00370.jpg        real_00587.jpg\t real_00804.jpg   real_01021.jpg\n",
            " real_00156.jpg   real_00371.jpg        real_00588.jpg\t real_00805.jpg   real_01022.jpg\n",
            " real_00157.jpg   real_00372.jpg        real_00589.jpg\t real_00806.jpg   real_01023.jpg\n",
            " real_00158.jpg   real_00373.jpg        real_00590.jpg\t real_00807.jpg   real_01024.jpg\n",
            " real_00159.jpg   real_00374.jpg        real_00591.jpg\t real_00808.jpg   real_01025.jpg\n",
            " real_00160.jpg   real_00375.jpg        real_00592.jpg\t real_00809.jpg   real_01026.jpg\n",
            " real_00161.jpg   real_00376.jpg        real_00593.jpg\t real_00810.jpg   real_01027.jpg\n",
            " real_00162.jpg   real_00377.jpg        real_00594.jpg\t real_00811.jpg   real_01028.jpg\n",
            " real_00163.jpg   real_00378.jpg        real_00595.jpg\t real_00812.jpg   real_01029.jpg\n",
            " real_00164.jpg   real_00379.jpg        real_00596.jpg\t real_00813.jpg   real_01030.jpg\n",
            " real_00165.jpg   real_00380.jpg        real_00597.jpg\t real_00814.jpg   real_01031.jpg\n",
            " real_00166.jpg   real_00381.jpg        real_00598.jpg\t real_00815.jpg   real_01032.jpg\n",
            " real_00167.jpg   real_00382.jpg        real_00599.jpg\t real_00816.jpg   real_01033.jpg\n",
            " real_00168.jpg   real_00383.jpg        real_00600.jpg\t real_00817.jpg   real_01034.jpg\n",
            " real_00169.jpg   real_00384.jpg        real_00601.jpg\t real_00818.jpg   real_01035.jpg\n",
            " real_00170.jpg   real_00385.jpg        real_00602.jpg\t real_00819.jpg   real_01036.jpg\n",
            " real_00171.jpg   real_00386.jpg        real_00603.jpg\t real_00820.jpg   real_01037.jpg\n",
            " real_00172.jpg   real_00387.jpg        real_00604.jpg\t real_00821.jpg   real_01038.jpg\n",
            " real_00173.jpg   real_00388.jpg        real_00605.jpg\t real_00822.jpg   real_01039.jpg\n",
            " real_00174.jpg   real_00389.jpg        real_00606.jpg\t real_00823.jpg   real_01040.jpg\n",
            " real_00175.jpg   real_00390.jpg        real_00607.jpg\t real_00824.jpg   real_01041.jpg\n",
            " real_00176.jpg   real_00391.jpg        real_00608.jpg\t real_00825.jpg   real_01042.jpg\n",
            " real_00177.jpg   real_00392.jpg        real_00609.jpg\t real_00826.jpg   real_01043.jpg\n",
            " real_00178.jpg   real_00393.jpg        real_00610.jpg\t real_00827.jpg   real_01044.jpg\n",
            " real_00179.jpg   real_00394.jpg        real_00611.jpg\t real_00828.jpg   real_01045.jpg\n",
            " real_00180.jpg   real_00395.jpg        real_00612.jpg\t real_00829.jpg   real_01046.jpg\n",
            " real_00181.jpg   real_00396.jpg        real_00613.jpg\t real_00830.jpg   real_01047.jpg\n",
            " real_00182.jpg   real_00397.jpg        real_00614.jpg\t real_00831.jpg   real_01048.jpg\n",
            " real_00183.jpg   real_00398.jpg        real_00615.jpg\t real_00832.jpg   real_01049.jpg\n",
            " real_00184.jpg   real_00399.jpg        real_00616.jpg\t real_00833.jpg   real_01050.jpg\n",
            " real_00185.jpg   real_00400.jpg        real_00617.jpg\t real_00834.jpg   real_01051.jpg\n",
            " real_00186.jpg   real_00401.jpg        real_00618.jpg\t real_00835.jpg   real_01052.jpg\n",
            " real_00187.jpg   real_00402.jpg        real_00619.jpg\t real_00836.jpg   real_01053.jpg\n",
            " real_00188.jpg   real_00403.jpg        real_00620.jpg\t real_00837.jpg   real_01054.jpg\n",
            " real_00189.jpg   real_00404.jpg        real_00621.jpg\t real_00838.jpg   real_01055.jpg\n",
            " real_00190.jpg   real_00405.jpg        real_00622.jpg\t real_00839.jpg   real_01056.jpg\n",
            " real_00191.jpg   real_00406.jpg        real_00623.jpg\t real_00840.jpg   real_01057.jpg\n",
            " real_00192.jpg   real_00407.jpg        real_00624.jpg\t real_00841.jpg   real_01058.jpg\n",
            " real_00193.jpg   real_00408.jpg        real_00625.jpg\t real_00842.jpg   real_01059.jpg\n",
            " real_00194.jpg   real_00409.jpg        real_00626.jpg\t real_00843.jpg   real_01060.jpg\n",
            " real_00195.jpg   real_00410.jpg        real_00627.jpg\t real_00844.jpg   real_01061.jpg\n",
            " real_00196.jpg   real_00411.jpg        real_00628.jpg\t real_00845.jpg   real_01062.jpg\n",
            " real_00197.jpg   real_00412.jpg        real_00629.jpg\t real_00846.jpg   real_01063.jpg\n",
            " real_00198.jpg   real_00413.jpg        real_00630.jpg\t real_00847.jpg   real_01064.jpg\n",
            " real_00199.jpg   real_00414.jpg        real_00631.jpg\t real_00848.jpg   real_01065.jpg\n",
            " real_00200.jpg   real_00415.jpg        real_00632.jpg\t real_00849.jpg   real_01066.jpg\n",
            " real_00201.jpg   real_00416.jpg        real_00633.jpg\t real_00850.jpg   real_01067.jpg\n",
            " real_00202.jpg   real_00417.jpg        real_00634.jpg\t real_00851.jpg   real_01068.jpg\n",
            " real_00203.jpg   real_00418.jpg        real_00635.jpg\t real_00852.jpg   real_01069.jpg\n",
            " real_00204.jpg   real_00419.jpg        real_00636.jpg\t real_00853.jpg   real_01070.jpg\n",
            " real_00205.jpg   real_00420.jpg        real_00637.jpg\t real_00854.jpg   real_01071.jpg\n",
            " real_00206.jpg   real_00421.jpg        real_00638.jpg\t real_00855.jpg   real_01072.jpg\n",
            " real_00207.jpg   real_00422.jpg        real_00639.jpg\t real_00856.jpg   real_01073.jpg\n",
            " real_00208.jpg   real_00423.jpg        real_00640.jpg\t real_00857.jpg   real_01074.jpg\n",
            " real_00209.jpg   real_00424.jpg        real_00641.jpg\t real_00858.jpg   real_01075.jpg\n",
            " real_00210.jpg   real_00425.jpg        real_00642.jpg\t real_00859.jpg   real_01076.jpg\n",
            " real_00211.jpg   real_00426.jpg        real_00643.jpg\t real_00860.jpg   real_01077.jpg\n",
            " real_00212.jpg   real_00427.jpg        real_00644.jpg\t real_00861.jpg   real_01078.jpg\n",
            " real_00213.jpg   real_00428.jpg        real_00645.jpg\t real_00862.jpg   real_01079.jpg\n",
            " real_00214.jpg   real_00429.jpg        real_00646.jpg\t real_00863.jpg   real_01080.jpg\n",
            " real_00215.jpg   real_00430.jpg        real_00647.jpg\t real_00864.jpg   real_01081.jpg\n",
            " real_00216.jpg   real_00431.jpg        real_00648.jpg\t real_00865.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace 'your_folder_name' with the actual folder name in your MyDrive directory\n",
        "!ls /content/drive/MyDrive/fake/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBJ1X4lNxifg",
        "outputId": "dfd3e38d-f3ec-40b8-8a8d-8fe80df1e481",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " easy_10_0001.jpg\t     easy_98_0001.jpg\t maxdefault.jpg     mid_316_1111.jpg\n",
            " easy_100_1111.jpg\t     easy_99_0101.jpg\t mid_100_1110.jpg   mid_317_1101.jpg\n",
            " easy_101_0010.jpg\t     hard_100_1111.jpg\t mid_101_1111.jpg   mid_318_1111.jpg\n",
            " easy_102_0101.jpg\t     hard_101_0011.jpg\t mid_10_1111.jpg    mid_319_1111.jpg\n",
            " easy_103_1111.jpg\t     hard_10_1111.jpg\t mid_102_1100.jpg   mid_320_1111.jpg\n",
            " easy_104_1000.jpg\t     hard_102_1001.jpg\t mid_103_1111.jpg   mid_321_1110.jpg\n",
            " easy_105_1100.jpg\t     hard_103_1101.jpg\t mid_104_1100.jpg   mid_32_1111.jpg\n",
            " easy_106_0011.jpg\t     hard_104_1001.jpg\t mid_105_1101.jpg   mid_322_1111.jpg\n",
            " easy_107_1011.jpg\t     hard_105_1011.jpg\t mid_106_0001.jpg   mid_323_1101.jpg\n",
            " easy_108_1111.jpg\t     hard_106_1100.jpg\t mid_107_0010.jpg   mid_324_1111.jpg\n",
            " easy_109_1111.jpg\t     hard_107_1111.jpg\t mid_108_1110.jpg   mid_325_0011.jpg\n",
            " easy_110_0001.jpg\t     hard_108_1111.jpg\t mid_109_1111.jpg   mid_326_1111.jpg\n",
            " easy_1_1110.jpg\t     hard_109_1100.jpg\t mid_110_1100.jpg   mid_327_0111.jpg\n",
            " easy_111_1101.jpg\t     hard_110_1110.jpg\t mid_111_0001.jpg   mid_328_0011.jpg\n",
            " easy_11_1111.jpg\t     hard_11_1100.jpg\t mid_1_1101.jpg     mid_329_1110.jpg\n",
            " easy_112_1001.jpg\t     hard_111_1111.jpg\t mid_11_1100.jpg    mid_330_1111.jpg\n",
            " easy_113_0011.jpg\t     hard_1_1111.jpg\t mid_112_0001.jpg   mid_331_0011.jpg\n",
            " easy_114_1110.jpg\t     hard_112_1111.jpg\t mid_113_1100.jpg   mid_33_1011.jpg\n",
            " easy_115_0010.jpg\t     hard_113_0011.jpg\t mid_114_1110.jpg   mid_332_1111.jpg\n",
            " easy_116_111.jpg\t     hard_114_1111.jpg\t mid_115_0011.jpg   mid_333_1101.jpg\n",
            " easy_117_0101.jpg\t     hard_115_1110.jpg\t mid_116_1111.jpg   mid_334_1111.jpg\n",
            " easy_118_1111.jpg\t     hard_116_1101.jpg\t mid_117_1111.jpg   mid_335_1100.jpg\n",
            " easy_119_0011.jpg\t     hard_117_1110.jpg\t mid_118_1000.jpg   mid_336_1111.jpg\n",
            " easy_120_0011.jpg\t     hard_118_0111.jpg\t mid_119_1111.jpg   mid_337_1111.jpg\n",
            " easy_121_0011.jpg\t     hard_119_1100.jpg\t mid_120_1100.jpg   mid_338_1100.jpg\n",
            " easy_12_1110.jpg\t     hard_120_0011.jpg\t mid_121_1110.jpg   mid_339_1100.jpg\n",
            " easy_122_1011.jpg\t     hard_12_1101.jpg\t mid_12_1111.jpg    mid_340_1111.jpg\n",
            " easy_123_0100.jpg\t     hard_121_1100.jpg\t mid_122_1100.jpg   mid_341_1110.jpg\n",
            " easy_124_1110.jpg\t     hard_122_1100.jpg\t mid_123_1111.jpg   mid_34_1111.jpg\n",
            " easy_125_0011.jpg\t     hard_123_1111.jpg\t mid_124_1110.jpg   mid_342_1110.jpg\n",
            " easy_126_0111.jpg\t     hard_124_1100.jpg\t mid_125_1111.jpg   mid_343_1111.jpg\n",
            " easy_127_0001.jpg\t     hard_125_1100.jpg\t mid_126_1110.jpg   mid_344_0010.jpg\n",
            " easy_128_0111.jpg\t     hard_126_0111.jpg\t mid_127_0011.jpg   mid_345_1111.jpg\n",
            " easy_129_1111.jpg\t     hard_127_0011.jpg\t mid_128_1110.jpg   mid_346_0011.jpg\n",
            " easy_130_1101.jpg\t     hard_128_1111.jpg\t mid_129_0101.jpg   mid_347_0011.jpg\n",
            " easy_13_1010.jpg\t     hard_129_1111.jpg\t mid_130_1111.jpg   mid_348_1100.jpg\n",
            " easy_131_1001.jpg\t     hard_130_1111.jpg\t mid_131_1111.jpg   mid_349_1111.jpg\n",
            " easy_132_0110.jpg\t     hard_131_1000.jpg\t mid_13_1111.jpg    mid_350_0100.jpg\n",
            " easy_133_1100.jpg\t     hard_13_1111.jpg\t mid_132_0001.jpg   mid_351_0110.jpg\n",
            " easy_134_0001.jpg\t     hard_132_0111.jpg\t mid_133_1001.jpg   mid_35_1101.jpg\n",
            " easy_135_0011.jpg\t     hard_133_0001.jpg\t mid_134_1010.jpg   mid_352_1111.jpg\n",
            " easy_136_0111.jpg\t     hard_134_1100.jpg\t mid_135_1110.jpg   mid_353_0011.jpg\n",
            " easy_137_0001.jpg\t     hard_135_1101.jpg\t mid_136_0001.jpg   mid_354_1110.jpg\n",
            " easy_138_0101.jpg\t     hard_136_1101.jpg\t mid_137_1011.jpg   mid_355_1011.jpg\n",
            " easy_139_0110.jpg\t     hard_137_1001.jpg\t mid_138_0011.jpg   mid_356_1110.jpg\n",
            " easy_140_1111.jpg\t     hard_138_1101.jpg\t mid_139_1111.jpg   mid_357_0011.jpg\n",
            " easy_141_0111.jpg\t     hard_139_1011.jpg\t mid_140_1111.jpg   mid_358_1100.jpg\n",
            " easy_14_1111.jpg\t     hard_140_1111.jpg\t mid_14_1101.jpg    mid_359_1101.jpg\n",
            " easy_142_1111.jpg\t     hard_141_0100.jpg\t mid_141_1110.jpg   mid_360_1100.jpg\n",
            " easy_143_0011.jpg\t     hard_14_1111.jpg\t mid_142_1100.jpg   mid_361_0001.jpg\n",
            " easy_144_1111.jpg\t     hard_142_1110.jpg\t mid_143_1100.jpg   mid_36_1100.jpg\n",
            " easy_145_1000.jpg\t     hard_143_0100.jpg\t mid_144_1110.jpg   mid_362_1101.jpg\n",
            " easy_146_1110.jpg\t     hard_144_0011.jpg\t mid_145_1111.jpg   mid_363_1101.jpg\n",
            " easy_147_1110.jpg\t     hard_145_1100.jpg\t mid_146_1111.jpg   mid_364_1111.jpg\n",
            " easy_148_0011.jpg\t     hard_146_0110.jpg\t mid_147_1110.jpg   mid_365_1111.jpg\n",
            " easy_149_1001.jpg\t     hard_147_1111.jpg\t mid_148_0001.jpg   mid_366_1111.jpg\n",
            " easy_150_0011.jpg\t     hard_148_1010.jpg\t mid_149_0011.jpg   mid_367_0010.jpg\n",
            " easy_15_0011.jpg\t     hard_149_0011.jpg\t mid_150_1111.jpg   mid_368_0110.jpg\n",
            " easy_151_1110.jpg\t     hard_150_1000.jpg\t mid_151_1100.jpg   mid_369_1110.jpg\n",
            " easy_152_0011.jpg\t     hard_151_0101.jpg\t mid_15_1111.jpg    mid_370_1111.jpg\n",
            " easy_153_1110.jpg\t     hard_15_1111.jpg\t mid_152_1100.jpg   mid_371_0011.jpg\n",
            " easy_154_1100.jpg\t     hard_152_0111.jpg\t mid_153_1111.jpg   mid_37_1111.jpg\n",
            " easy_155_0011.jpg\t     hard_153_1011.jpg\t mid_154_1111.jpg   mid_372_0011.jpg\n",
            " easy_156_0010.jpg\t     hard_154_1100.jpg\t mid_155_1100.jpg   mid_373_1111.jpg\n",
            " easy_157_1110.jpg\t     hard_155_0010.jpg\t mid_156_1111.jpg   mid_374_0011.jpg\n",
            " easy_158_0011.jpg\t     hard_156_1111.jpg\t mid_157_1111.jpg   mid_375_1110.jpg\n",
            " easy_159_1111.jpg\t     hard_157_1111.jpg\t mid_158_1010.jpg   mid_376_0001.jpg\n",
            " easy_160_0011.jpg\t     hard_158_0011.jpg\t mid_159_1111.jpg   mid_377_0100.jpg\n",
            " easy_161_1101.jpg\t     hard_159_0100.jpg\t mid_160_1110.jpg   mid_378_1110.jpg\n",
            " easy_16_1111.jpg\t     hard_160_1111.jpg\t mid_161_0110.jpg   mid_379_1100.jpg\n",
            " easy_162_0001.jpg\t     hard_161_1100.jpg\t mid_16_1111.jpg    mid_380_1111.jpg\n",
            " easy_163_1001.jpg\t     hard_16_1111.jpg\t mid_162_0011.jpg   mid_381_1101.jpg\n",
            " easy_164_1111.jpg\t     hard_162_0010.jpg\t mid_163_1001.jpg   mid_38_1111.jpg\n",
            " easy_165_1010.jpg\t     hard_163_0011.jpg\t mid_164_0011.jpg   mid_382_0011.jpg\n",
            " easy_166_1100.jpg\t     hard_164_1000.jpg\t mid_165_0101.jpg   mid_383_1100.jpg\n",
            " easy_167_0110.jpg\t     hard_165_1000.jpg\t mid_166_1100.jpg   mid_384_0011.jpg\n",
            " easy_168_0010.jpg\t     hard_166_1110.jpg\t mid_167_1100.jpg   mid_385_1100.jpg\n",
            " easy_169_1100.jpg\t     hard_167_0011.jpg\t mid_168_1111.jpg   mid_386_1110.jpg\n",
            " easy_170_0010.jpg\t     hard_168_0011.jpg\t mid_169_0001.jpg   mid_387_1100.jpg\n",
            " easy_17_0011.jpg\t     hard_169_0111.jpg\t mid_170_1111.jpg   mid_388_1111.jpg\n",
            " easy_171_1100.jpg\t     hard_170_0111.jpg\t mid_171_1100.jpg   mid_389_1101.jpg\n",
            " easy_172_1001.jpg\t     hard_17_1010.jpg\t mid_17_1111.jpg    mid_390_0011.jpg\n",
            " easy_173_0111.jpg\t     hard_171_1101.jpg\t mid_172_1111.jpg   mid_391_1110.jpg\n",
            " easy_174_1110.jpg\t     hard_172_1100.jpg\t mid_173_1110.jpg   mid_39_1111.jpg\n",
            " easy_175_0001.jpg\t     hard_173_1110.jpg\t mid_174_1111.jpg   mid_392_1111.jpg\n",
            " easy_176_1000.jpg\t     hard_174_1111.jpg\t mid_175_1111.jpg   mid_393_1101.jpg\n",
            " easy_177_1010.jpg\t     hard_175_1100.jpg\t mid_176_1111.jpg   mid_394_1111.jpg\n",
            " easy_178_1111.jpg\t     hard_176_0011.jpg\t mid_177_1111.jpg   mid_395_1110.jpg\n",
            " easy_179_0001.jpg\t     hard_177_1011.jpg\t mid_178_1111.jpg   mid_396_1011.jpg\n",
            " easy_180_0011.jpg\t     hard_178_1001.jpg\t mid_179_1111.jpg   mid_397_1111.jpg\n",
            " easy_18_0011.jpg\t     hard_179_1111.jpg\t mid_180_1110.jpg   mid_398_0011.jpg\n",
            " easy_181_1110.jpg\t     hard_180_0011.jpg\t mid_181_0011.jpg   mid_399_0011.jpg\n",
            " easy_182_0100.jpg\t     hard_181_0001.jpg\t mid_18_1111.jpg    mid_400_0011.jpg\n",
            " easy_183_1111.jpg\t     hard_18_1111.jpg\t mid_182_1111.jpg   mid_401_0111.jpg\n",
            " easy_184_0110.jpg\t     hard_182_0011.jpg\t mid_183_1111.jpg   mid_40_1111.jpg\n",
            " easy_185_1101.jpg\t     hard_183_1101.jpg\t mid_184_1101.jpg   mid_402_0011.jpg\n",
            " easy_186_1001.jpg\t     hard_184_0011.jpg\t mid_185_1110.jpg   mid_403_1111.jpg\n",
            " easy_187_1110.jpg\t     hard_185_1100.jpg\t mid_186_1100.jpg   mid_404_0011.jpg\n",
            " easy_188_1111.jpg\t     hard_186_0011.jpg\t mid_187_1101.jpg   mid_405_0011.jpg\n",
            " easy_189_1101.jpg\t     hard_187_0100.jpg\t mid_188_1111.jpg   mid_406_0011.jpg\n",
            " easy_190_0101.jpg\t     hard_188_0100.jpg\t mid_189_0011.jpg   mid_407_1111.jpg\n",
            " easy_19_0111.jpg\t     hard_189_0011.jpg\t mid_190_1111.jpg   mid_408_1111.jpg\n",
            " easy_191_0001.jpg\t     hard_190_1111.jpg\t mid_19_1000.jpg    mid_409_1101.jpg\n",
            " easy_192_1000.jpg\t     hard_191_0011.jpg\t mid_191_0111.jpg   mid_410_1111.jpg\n",
            " easy_193_1110.jpg\t     hard_19_1111.jpg\t mid_192_1111.jpg   mid_411_1111.jpg\n",
            " easy_194_1011.jpg\t     hard_192_1111.jpg\t mid_193_1111.jpg   mid_41_1111.jpg\n",
            " easy_195_1110.jpg\t     hard_193_1111.jpg\t mid_194_1111.jpg   mid_4_1111.jpg\n",
            " easy_196_0010.jpg\t     hard_194_0110.jpg\t mid_195_1111.jpg   mid_412_0011.jpg\n",
            " easy_197_1111.jpg\t     hard_195_1000.jpg\t mid_196_1100.jpg   mid_413_1111.jpg\n",
            " easy_198_0011.jpg\t     hard_196_1101.jpg\t mid_197_1110.jpg   mid_414_0011.jpg\n",
            " easy_199_1100.jpg\t     hard_197_1100.jpg\t mid_198_1111.jpg   mid_415_1111.jpg\n",
            " easy_200_1110.jpg\t     hard_198_1100.jpg\t mid_199_0011.jpg   mid_416_1111.jpg\n",
            " easy_201_0001.jpg\t     hard_199_0011.jpg\t mid_200_1111.jpg   mid_417_1111.jpg\n",
            " easy_20_1111.jpg\t     hard_200_1111.jpg\t mid_201_1111.jpg   mid_418_1101.jpg\n",
            " easy_202_0011.jpg\t     hard_201_0100.jpg\t mid_20_1111.jpg    mid_419_0011.jpg\n",
            " easy_203_0111.jpg\t     hard_20_1111.jpg\t mid_202_1110.jpg   mid_420_1101.jpg\n",
            " easy_204_0101.jpg\t     hard_202_1101.jpg\t mid_203_1111.jpg   mid_421_0011.jpg\n",
            " easy_205_1110.jpg\t     hard_203_0010.jpg\t mid_204_1111.jpg   mid_42_1111.jpg\n",
            " easy_206_0011.jpg\t     hard_204_0011.jpg\t mid_205_0011.jpg   mid_422_0011.jpg\n",
            " easy_207_1000.jpg\t     hard_205_0011.jpg\t mid_206_1111.jpg   mid_423_1101.jpg\n",
            " easy_208_1111.jpg\t     hard_206_0001.jpg\t mid_207_1111.jpg   mid_424_1111.jpg\n",
            " easy_209_0001.jpg\t     hard_207_1100.jpg\t mid_208_1110.jpg   mid_425_1111.jpg\n",
            " easy_21_0011.jpg\t     hard_208_0001.jpg\t mid_209_1111.jpg   mid_426_1111.jpg\n",
            " easy_210_1011.jpg\t     hard_209_1011.jpg\t mid_210_0011.jpg   mid_427_1010.jpg\n",
            " easy_211_1100.jpg\t     hard_210_1100.jpg\t mid_21_0011.jpg    mid_428_0001.jpg\n",
            " easy_2_1111.jpg\t     hard_211_0110.jpg\t mid_2_1101.jpg     mid_429_1110.jpg\n",
            " easy_212_0010.jpg\t     hard_21_1101.jpg\t mid_211_1100.jpg   mid_430_1111.jpg\n",
            " easy_213_1111.jpg\t     hard_2_1111.jpg\t mid_212_1111.jpg   mid_431_0001.jpg\n",
            " easy_214_0011.jpg\t     hard_212_1101.jpg\t mid_213_1111.jpg   mid_43_1111.jpg\n",
            " easy_215_1111.jpg\t     hard_213_0101.jpg\t mid_214_1011.jpg   mid_432_1111.jpg\n",
            " easy_216_1100.jpg\t     hard_214_1011.jpg\t mid_215_0011.jpg   mid_433_0011.jpg\n",
            " easy_217_1111.jpg\t     hard_215_1101.jpg\t mid_216_1011.jpg   mid_434_1110.jpg\n",
            " easy_218_0011.jpg\t     hard_216_0111.jpg\t mid_217_1111.jpg   mid_435_1111.jpg\n",
            " easy_219_1101.jpg\t     hard_217_1100.jpg\t mid_218_1011.jpg   mid_436_1111.jpg\n",
            " easy_220_1000.jpg\t     hard_218_0100.jpg\t mid_219_1111.jpg   mid_437_1101.jpg\n",
            " easy_22_1010.jpg\t     hard_219_1101.jpg\t mid_220_0011.jpg   mid_438_0010.jpg\n",
            " easy_221_0111.jpg\t     hard_220_1111.jpg\t mid_22_1101.jpg    mid_439_1000.jpg\n",
            " easy_222_0011.jpg\t     hard_221_0101.jpg\t mid_221_1100.jpg   mid_440_0101.jpg\n",
            " easy_223_0100.jpg\t     hard_22_1111.jpg\t mid_222_1110.jpg   mid_441_0011.jpg\n",
            " easy_224_1100.jpg\t     hard_222_1001.jpg\t mid_223_1111.jpg   mid_44_1111.jpg\n",
            " easy_225_0010.jpg\t     hard_223_1111.jpg\t mid_224_1111.jpg   mid_442_0111.jpg\n",
            " easy_226_1111.jpg\t     hard_224_0111.jpg\t mid_225_0111.jpg   mid_443_0011.jpg\n",
            " easy_227_1110.jpg\t     hard_225_0011.jpg\t mid_226_1111.jpg   mid_444_1111.jpg\n",
            " easy_228_0011.jpg\t     hard_226_0100.jpg\t mid_227_1111.jpg   mid_445_1111.jpg\n",
            " easy_229_1000.jpg\t     hard_227_1100.jpg\t mid_228_1111.jpg   mid_446_0011.jpg\n",
            " easy_230_1010.jpg\t     hard_228_1111.jpg\t mid_229_1111.jpg   mid_447_0011.jpg\n",
            " easy_23_1100.jpg\t     hard_229_0100.jpg\t mid_230_1110.jpg   mid_448_0011.jpg\n",
            " easy_231_1011.jpg\t     hard_230_1110.jpg\t mid_231_1100.jpg   mid_449_1111.jpg\n",
            " easy_232_1100.jpg\t     hard_23_1110.jpg\t mid_23_1111.jpg    mid_45_0010.jpg\n",
            " easy_233_0010.jpg\t     hard_231_1111.jpg\t mid_232_1111.jpg   mid_450_1101.jpg\n",
            " easy_234_1000.jpg\t     hard_232_0001.jpg\t mid_233_1111.jpg   mid_451_1111.jpg\n",
            " easy_235_1111.jpg\t     hard_233_0010.jpg\t mid_234_0010.jpg   mid_452_1111.jpg\n",
            " easy_236_1100.jpg\t     hard_234_1100.jpg\t mid_235_1100.jpg   mid_453_1111.jpg\n",
            " easy_237_0011.jpg\t     hard_235_0001.jpg\t mid_236_1111.jpg   mid_454_1111.jpg\n",
            " easy_238_1111.jpg\t     hard_236_1001.jpg\t mid_237_1111.jpg   mid_455_0101.jpg\n",
            " easy_239_1110.jpg\t     hard_237_0010.jpg\t mid_238_1111.jpg   mid_456_1111.jpg\n",
            " easy_24_0011.jpg\t     hard_238_1000.jpg\t mid_239_1011.jpg   mid_457_1100.jpg\n",
            " easy_240_1110.jpg\t     hard_239_1000.jpg\t mid_240_1100.jpg   mid_458_1111.jpg\n",
            " easy_25_0110.jpg\t     hard_240_1101.jpg\t mid_241_1111.jpg   mid_459_1001.jpg\n",
            " easy_26_1100.jpg\t     hard_24_1100.jpg\t mid_24_1111.jpg    mid_460_1111.jpg\n",
            " easy_27_1000.jpg\t     hard_25_1111.jpg\t mid_242_1111.jpg   mid_461_1100.jpg\n",
            " easy_28_1100.jpg\t     hard_26_0011.jpg\t mid_243_0011.jpg   mid_46_1111.jpg\n",
            " easy_29_0011.jpg\t     hard_27_1111.jpg\t mid_244_1110.jpg   mid_462_1101.jpg\n",
            " easy_30_1001.jpg\t     hard_28_1111.jpg\t mid_245_1100.jpg   mid_463_0100.jpg\n",
            " easy_31_0011.jpg\t     hard_29_1111.jpg\t mid_246_0011.jpg   mid_464_1111.jpg\n",
            " easy_3_1100.jpg\t     hard_30_1101.jpg\t mid_247_1111.jpg   mid_465_1111.jpg\n",
            " easy_32_1100.jpg\t     hard_31_0011.jpg\t mid_248_0111.jpg   mid_466_1111.jpg\n",
            " easy_33_0010.jpg\t     hard_3_1111.jpg\t mid_249_1111.jpg   mid_467_1111.jpg\n",
            " easy_34_1100.jpg\t     hard_32_1111.jpg\t mid_250_1111.jpg   mid_468_1110.jpg\n",
            " easy_35_0110.jpg\t     hard_33_1011.jpg\t mid_251_1111.jpg   mid_469_1111.jpg\n",
            " easy_36_1111.jpg\t     hard_34_1111.jpg\t mid_25_1111.jpg    mid_470_1001.jpg\n",
            " easy_37_0010.jpg\t     hard_35_1111.jpg\t mid_252_1101.jpg   mid_47_1110.jpg\n",
            " easy_38_0010.jpg\t     hard_36_1101.jpg\t mid_253_0101.jpg   mid_471_1111.jpg\n",
            " easy_39_0011.jpg\t     hard_37_1111.jpg\t mid_254_1111.jpg   mid_472_1101.jpg\n",
            " easy_4_0011.jpg\t     hard_38_1110.jpg\t mid_255_0011.jpg   mid_473_0011.jpg\n",
            " easy_40_1111.jpg\t     hard_39_1111.jpg\t mid_256_1100.jpg   mid_474_1110.jpg\n",
            " easy_41_0001.jpg\t     hard_40_1111.jpg\t mid_257_1111.jpg   mid_475_1111.jpg\n",
            " easy_42_0001.jpg\t     hard_41_1111.jpg\t mid_258_1110.jpg   mid_476_1111.jpg\n",
            " easy_43_1100.jpg\t     hard_4_1111.jpg\t mid_259_1101.jpg   mid_477_1100.jpg\n",
            " easy_44_0011.jpg\t     hard_42_1111.jpg\t mid_260_1111.jpg   mid_478_1101.jpg\n",
            " easy_45_1010.jpg\t     hard_43_1111.jpg\t mid_26_1101.jpg    mid_479_1111.jpg\n",
            " easy_46_1100.jpg\t     hard_44_1111.jpg\t mid_261_1101.jpg   mid_480_1111.jpg\n",
            " easy_47_1110.jpg\t     hard_45_1111.jpg\t mid_262_1111.jpg   mid_48_1111.jpg\n",
            " easy_48_0011.jpg\t     hard_46_0011.jpg\t mid_263_1100.jpg   mid_49_1111.jpg\n",
            " easy_49_1000.jpg\t     hard_47_1100.jpg\t mid_264_1101.jpg   mid_5_0011.jpg\n",
            " easy_50_0110.jpg\t     hard_48_0011.jpg\t mid_265_1111.jpg   mid_50_1111.jpg\n",
            " easy_5_1100.jpg\t     hard_49_0010.jpg\t mid_266_1101.jpg   mid_51_1111.jpg\n",
            " easy_51_1110.jpg\t     hard_50_1101.jpg\t mid_267_1111.jpg   mid_52_1111.jpg\n",
            " easy_52_1100.jpg\t     hard_51_1111.jpg\t mid_268_0010.jpg   mid_53_1100.jpg\n",
            " easy_53_0001.jpg\t     hard_5_1111.jpg\t mid_269_1100.jpg   mid_54_1111.jpg\n",
            " easy_54_1010.jpg\t     hard_52_1111.jpg\t mid_270_0101.jpg   mid_55_1111.jpg\n",
            " easy_55_0011.jpg\t     hard_53_1100.jpg\t mid_271_1111.jpg   mid_56_1111.jpg\n",
            " easy_56_1110.jpg\t     hard_54_0001.jpg\t mid_27_1111.jpg    mid_57_1111.jpg\n",
            " easy_57_0010.jpg\t     hard_55_0011.jpg\t mid_272_1010.jpg   mid_58_1100.jpg\n",
            " easy_58_0011.jpg\t     hard_56_0100.jpg\t mid_273_1101.jpg   mid_59_1111.jpg\n",
            " easy_59_0010.jpg\t     hard_57_0110.jpg\t mid_274_1100.jpg   mid_60_1101.jpg\n",
            " easy_60_1111.jpg\t     hard_58_0001.jpg\t mid_275_1100.jpg   mid_61_1111.jpg\n",
            " easy_61_0111.jpg\t     hard_59_1110.jpg\t mid_276_0011.jpg   mid_6_1111.jpg\n",
            " easy_6_1110.jpg\t     hard_60_1111.jpg\t mid_277_1100.jpg   mid_62_1110.jpg\n",
            " easy_62_0001.jpg\t     hard_61_0011.jpg\t mid_278_1101.jpg   mid_63_1111.jpg\n",
            " easy_63_1110.jpg\t     hard_6_1110.jpg\t mid_279_1111.jpg   mid_64_1100.jpg\n",
            " easy_64_1111.jpg\t     hard_62_1010.jpg\t mid_280_1111.jpg   mid_65_1111.jpg\n",
            " easy_65_1111.jpg\t     hard_63_1101.jpg\t mid_281_1111.jpg   mid_66_1100.jpg\n",
            " easy_66_1101.jpg\t     hard_64_0010.jpg\t mid_28_1111.jpg    mid_67_1111.jpg\n",
            " easy_67_0101.jpg\t     hard_65_0101.jpg\t mid_282_1100.jpg   mid_68_1111.jpg\n",
            " easy_68_1100.jpg\t     hard_66_1111.jpg\t mid_283_0111.jpg   mid_69_1111.jpg\n",
            " easy_69_1011.jpg\t     hard_67_0001.jpg\t mid_284_1110.jpg   mid_70_1111.jpg\n",
            " easy_70_0011.jpg\t     hard_68_1100.jpg\t mid_285_1111.jpg   mid_7_1100.jpg\n",
            " easy_7_1100.jpg\t     hard_69_0011.jpg\t mid_286_1100.jpg   mid_71_1111.jpg\n",
            " easy_71_1100.jpg\t     hard_70_1100.jpg\t mid_287_0001.jpg   mid_72_1111.jpg\n",
            " easy_72_1111.jpg\t     hard_71_1100.jpg\t mid_288_1101.jpg   mid_73_1110.jpg\n",
            " easy_73_1110.jpg\t     hard_7_1111.jpg\t mid_289_1101.jpg   mid_74_1111.jpg\n",
            " easy_74_0010.jpg\t     hard_72_1100.jpg\t mid_290_1100.jpg   mid_75_1111.jpg\n",
            " easy_75_1100.jpg\t     hard_73_0011.jpg\t mid_291_1100.jpg   mid_76_1111.jpg\n",
            " easy_76_1111.jpg\t     hard_74_0011.jpg\t mid_29_1111.jpg    mid_77_1110.jpg\n",
            " easy_77_0111.jpg\t     hard_75_0011.jpg\t mid_292_1000.jpg   mid_78_1101.jpg\n",
            " easy_78_1111.jpg\t     hard_76_1100.jpg\t mid_293_1101.jpg   mid_79_1111.jpg\n",
            " easy_79_0001.jpg\t     hard_77_1100.jpg\t mid_294_1110.jpg   mid_80_1111.jpg\n",
            " easy_80_0001.jpg\t     hard_78_1101.jpg\t mid_295_0111.jpg   mid_81_1101.jpg\n",
            " easy_8_0010.jpg\t     hard_79_1110.jpg\t mid_296_1111.jpg   mid_8_1111.jpg\n",
            "'easy_81_1111 (1).jpg'\t     hard_80_0100.jpg\t mid_297_0010.jpg   mid_82_1111.jpg\n",
            "'easy_81_1111 (2).jpg'\t     hard_8_1000.jpg\t mid_298_0001.jpg   mid_83_1111.jpg\n",
            " easy_81_1111.jpg\t     hard_81_1101.jpg\t mid_299_1100.jpg   mid_84_1111.jpg\n",
            " easy_82_0001.jpg\t     hard_82_0011.jpg\t mid_300_0111.jpg   mid_85_0011.jpg\n",
            " easy_83_1110.jpg\t     hard_83_1111.jpg\t mid_301_0101.jpg   mid_86_1111.jpg\n",
            "'easy_84_0011 (1) (1).jpg'   hard_84_0011.jpg\t mid_30_1110.jpg    mid_87_1111.jpg\n",
            "'easy_84_0011 (1).jpg'\t     hard_85_1111.jpg\t mid_302_1100.jpg   mid_88_1110.jpg\n",
            " easy_84_0011.jpg\t     hard_86_1100.jpg\t mid_303_1100.jpg   mid_89_1111.jpg\n",
            " easy_85_1111.jpg\t     hard_87_1111.jpg\t mid_304_1111.jpg   mid_90_1111.jpg\n",
            " easy_86_0011.jpg\t     hard_88_1111.jpg\t mid_305_1101.jpg   mid_9_1110.jpg\n",
            " easy_87_0110.jpg\t     hard_89_1110.jpg\t mid_306_1111.jpg   mid_91_1111.jpg\n",
            " easy_88_0011.jpg\t     hard_90_1100.jpg\t mid_307_1101.jpg   mid_92_1111.jpg\n",
            " easy_89_0010.jpg\t     hard_91_1100.jpg\t mid_308_1111.jpg   mid_93_1111.jpg\n",
            " easy_90_1111.jpg\t     hard_9_1111.jpg\t mid_309_1111.jpg   mid_94_1111.jpg\n",
            " easy_9_1010.jpg\t     hard_92_1001.jpg\t mid_310_0111.jpg   mid_95_1100.jpg\n",
            " easy_91_1000.jpg\t     hard_93_1111.jpg\t mid_311_0011.jpg   mid_96_1111.jpg\n",
            " easy_92_0011.jpg\t     hard_94_1111.jpg\t mid_3_1100.jpg     mid_97_1111.jpg\n",
            " easy_93_1010.jpg\t     hard_95_0001.jpg\t mid_31_1111.jpg    mid_98_1111.jpg\n",
            " easy_94_0001.jpg\t     hard_96_1110.jpg\t mid_312_1111.jpg   mid_99_1100.jpg\n",
            " easy_95_0101.jpg\t     hard_97_1001.jpg\t mid_313_1101.jpg  'pandu AI.png'\n",
            " easy_96_0011.jpg\t     hard_98_1101.jpg\t mid_314_1101.jpg   pic.png\n",
            " easy_97_1111.jpg\t     hard_99_1111.jpg\t mid_315_1101.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/projectpics/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwDhtkLBq6J6",
        "outputId": "8fd113fa-1d15-41ed-a34e-5a873e21c0a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " download.jpeg\t maxdefault.jpg  'pandu AI.png'   pic.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import albumentations as A\n",
        "\n",
        "class DeepfakeDataset(Dataset):\n",
        "    def __init__(self, real_dir, fake_dir, transform=None):\n",
        "        self.real_images = [os.path.join(real_dir, img) for img in os.listdir(real_dir)]\n",
        "        self.fake_images = [os.path.join(fake_dir, img) for img in os.listdir(fake_dir)]\n",
        "        self.all_images = self.real_images + self.fake_images\n",
        "        self.labels = [0] * len(self.real_images) + [1] * len(self.fake_images)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.all_images[idx]\n",
        "        image = cv2.imread(img_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image=image)['image']\n",
        "        return image, label\n",
        "\n",
        "transform = A.Compose([\n",
        "    A.Resize(224, 224),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "# Update paths to point to your dataset in Google Drive\n",
        "real_dir = '/content/drive/MyDrive/dataset/real'\n",
        "fake_dir = '/content/drive/MyDrive/dataset/fake'\n",
        "\n",
        "# Make sure to replace 'real' and 'fake' with the correct paths from your drive link\n",
        "real_dir = '/content/drive/MyDrive/real'\n",
        "fake_dir = '/content/drive/MyDrive/fake'\n",
        "\n",
        "dataset = DeepfakeDataset(real_dir, fake_dir, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "PKmjPxJ2Gd5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Data augmentation and normalization for training\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "# Define classes to keep (in this case, we will use classes 0 and 1)\n",
        "classes_to_keep = [0, 1]\n",
        "\n",
        "# Filter the dataset to include only the specified classes\n",
        "indices = [i for i, label in enumerate(train_dataset.targets) if label in classes_to_keep]\n",
        "subset_dataset = Subset(train_dataset, indices)\n",
        "\n",
        "# Create a new dataset with modified targets to be 0 or 1\n",
        "subset_dataset_modified = [(image, classes_to_keep.index(label)) for image, label in subset_dataset]\n",
        "\n",
        "# Create DataLoader with reduced num_workers and smaller batch size\n",
        "train_loader = DataLoader(subset_dataset_modified, batch_size=16, shuffle=True, num_workers=2)\n",
        "\n",
        "# Load a pre-trained ResNet-50 model\n",
        "model = models.resnet50(pretrained=True)\n",
        "num_features = model.fc.in_features\n",
        "model.fc = nn.Linear(num_features, 2)  # Output layer for 2 classes\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "try:\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for images, labels in tqdm(train_loader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
        "except RuntimeError as e:\n",
        "    print(f\"Runtime error: {e}\")\n",
        "\n",
        "# Save the model after training\n",
        "torch.save(model.state_dict(), 'resnet50_cifar10.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HN6VXz2GxhZv",
        "outputId": "4f758e6c-538a-4906-a40f-0b78f9ddea8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:03<00:00, 51846360.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 210MB/s]\n",
            "100%|██████████| 625/625 [05:07<00:00,  2.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.3013\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [05:05<00:00,  2.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/10], Loss: 0.1824\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [05:16<00:00,  1.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/10], Loss: 0.1437\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [05:08<00:00,  2.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/10], Loss: 0.1279\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [05:03<00:00,  2.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/10], Loss: 0.1026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [05:07<00:00,  2.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/10], Loss: 0.1306\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [05:06<00:00,  2.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/10], Loss: 0.0816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [05:07<00:00,  2.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/10], Loss: 0.0755\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [05:05<00:00,  2.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/10], Loss: 0.0655\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [05:03<00:00,  2.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/10], Loss: 0.0616\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define a simple model (example: a basic CNN)\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)  # 64 channels, 7x7 size after pooling\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.max_pool2d(x, kernel_size=2, stride=2)\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.max_pool2d(x, kernel_size=2, stride=2)\n",
        "        x = x.view(-1, 64 * 7 * 7)  # Flatten the tensor\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Create the model instance\n",
        "model = SimpleCNN()\n",
        "\n",
        "# Move the model to the appropriate device (GPU or CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Example dataloader (using MNIST dataset)\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Number of epochs to train\n",
        "num_epochs = 10\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in tqdm(dataloader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}')\n"
      ],
      "metadata": {
        "id": "2dJ32UsDt50R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31759797-7bd2-4060-fe57-624a4b006e0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 71532781.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 18763273.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 78875599.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 5395221.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 938/938 [01:46<00:00,  8.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.1608\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [01:38<00:00,  9.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/10], Loss: 0.0456\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [01:34<00:00,  9.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/10], Loss: 0.0318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [01:29<00:00, 10.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/10], Loss: 0.0231\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [01:35<00:00,  9.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/10], Loss: 0.0181\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [01:41<00:00,  9.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/10], Loss: 0.0142\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [01:32<00:00, 10.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/10], Loss: 0.0126\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [01:33<00:00, 10.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/10], Loss: 0.0087\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [01:37<00:00,  9.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/10], Loss: 0.0086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [01:36<00:00,  9.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/10], Loss: 0.0072\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Data augmentation and normalization for training\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "# Define classes to keep (in this case, we will use classes 0 and 1)\n",
        "classes_to_keep = [0, 1]\n",
        "\n",
        "# Filter the dataset to include only the specified classes\n",
        "indices = [i for i, label in enumerate(train_dataset.targets) if label in classes_to_keep]\n",
        "subset_dataset = Subset(train_dataset, indices)\n",
        "\n",
        "# Create a new dataset with modified targets to be 0 or 1\n",
        "subset_dataset_modified = [(image, classes_to_keep.index(label)) for image, label in subset_dataset]\n",
        "\n",
        "# Create DataLoader with reduced num_workers and smaller batch size\n",
        "train_loader = DataLoader(subset_dataset_modified, batch_size=16, shuffle=True, num_workers=2)\n",
        "\n",
        "# Load a pre-trained ResNet-50 model\n",
        "model = models.resnet50(pretrained=True)\n",
        "num_features = model.fc.in_features\n",
        "model.fc = nn.Linear(num_features, 2)  # Output layer for 2 classes\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "try:\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for images, labels in tqdm(train_loader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
        "except RuntimeError as e:\n",
        "    print(f\"Runtime error: {e}\")\n",
        "\n",
        "# Save the model after training\n",
        "torch.save(model.state_dict(), 'resnet50_cifar10.pth')\n"
      ],
      "metadata": {
        "id": "o_-agD1h5ki5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8092dca9-4a83-478f-d368-3b59b46cfeb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:12<00:00, 13207385.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 168MB/s]\n",
            "100%|██████████| 625/625 [01:48<00:00,  5.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.2840\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [01:47<00:00,  5.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/10], Loss: 0.1727\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [01:46<00:00,  5.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/10], Loss: 0.1348\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [01:46<00:00,  5.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/10], Loss: 0.1276\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [01:46<00:00,  5.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/10], Loss: 0.0934\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [01:46<00:00,  5.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/10], Loss: 0.0880\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [01:46<00:00,  5.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/10], Loss: 0.0737\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [01:46<00:00,  5.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/10], Loss: 0.0755\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [01:46<00:00,  5.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/10], Loss: 0.0657\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 625/625 [01:46<00:00,  5.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/10], Loss: 0.0558\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in dataloader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "report = classification_report(all_labels, all_preds)\n",
        "\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(f'Classification Report:\\n {report}')\n"
      ],
      "metadata": {
        "id": "nv9CizUK5lQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import cv2 # Import the cv2 library\n",
        "\n",
        "# Define a simple model (example: a basic CNN)\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)  # 64 channels, 7x7 size after pooling\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.max_pool2d(x, kernel_size=2, stride=2)\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.max_pool2d(x, kernel_size=2, stride=2)\n",
        "        x = x.view(-1, 64 * 7 * 7)  # Flatten the tensor\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Create the model instance\n"
      ],
      "metadata": {
        "id": "vvUdmbOhDa5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "\n",
        "# Assuming you have a pre-trained model for deepfake detection\n",
        "# You need to replace this with your actual model and its loading mechanism\n",
        "model = models.resnet18(pretrained=True)  # Example model\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, 2)  # Adjusting the final layer for binary classification\n",
        "\n",
        "# Load your model weights (if you have a checkpoint)\n",
        "# model.load_state_dict(torch.load('path_to_your_model_weights.pth'))\n",
        "model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Define the image transformation\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),  # Adjust according to your model's input size\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def detect_deepfake(model, image_path, transform):\n",
        "    model.eval()\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        raise ValueError(f\"Image not found or unable to load: {image_path}\")\n",
        "\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    transformed = transform(image)\n",
        "    transformed = transformed.unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(transformed)\n",
        "        _, pred = torch.max(output, 1)\n",
        "\n",
        "    return 'Fake' if pred.item() == 1 else 'Real'\n",
        "\n",
        "# Example usage\n",
        "image_path = '/content/drive/MyDrive/fake/.png'\n",
        "try:\n",
        "    result = detect_deepfake(model, image_path, transform)\n",
        "    print(f'The image is: {result}')\n",
        "except ValueError as e:\n",
        "    print(e)\n"
      ],
      "metadata": {
        "id": "q2XF1D10D5ju",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b661e27-1e1b-4de3-b188-31f09488cfcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 138MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The image is: Real\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the image path\n",
        "image_path = '/content/drive/MyDrive/fake/pic.png'\n",
        "\n",
        "# Perform inference\n",
        "result = detect_deepfake(model, image_path, transform)\n",
        "print(f'The image is: {result}')\n"
      ],
      "metadata": {
        "id": "anxZJF0yFgbb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b6e0c73-7a4c-40cd-861f-5fa2250014c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The image is: Real\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!ls /content/drive/MyDrive/videos/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7IrZDtsrq1N",
        "outputId": "a4512744-ff32-4e74-fa29-9159cb52cb59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "'jimmy fallon-fake.mp4'  '#jimmyfallon part 2 😂.mp4'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Verify the video file is present\n",
        "!ls /content/drive/MyDrive/videos/\n",
        "\n",
        "# Step 2: Import necessary libraries and define the model\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import albumentations as A\n",
        "\n",
        "# Define the transformation\n",
        "transform = A.Compose([\n",
        "    A.Resize(224, 224),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "# Load the model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = models.resnet50(pretrained=True)\n",
        "num_features = model.fc.in_features\n",
        "model.fc = nn.Linear(num_features, 2)  # Assuming binary classification (real or fake)\n",
        "model = model.to(device)\n",
        "\n",
        "# Uncomment if you have saved model weights\n",
        "# model.load_state_dict(torch.load('/path/to/saved_model.pth'))\n",
        "\n",
        "# Step 3: Define the video inference function\n",
        "def detect_deepfake_in_video(model, video_path, transform, device):\n",
        "    model.eval()\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    predictions = []\n",
        "\n",
        "    for _ in range(frame_count):\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        image = transform(image=image)['image']\n",
        "        image = image.unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(image)\n",
        "            _, pred = torch.max(output, 1)\n",
        "            predictions.append(pred.item())\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    # Determine if the video is mostly fake or real\n",
        "    fake_count = predictions.count(1)\n",
        "    real_count = predictions.count(0)\n",
        "    result = 'Fake' if fake_count > real_count else 'Real'\n",
        "\n",
        "    return result, fake_count, real_count\n",
        "\n",
        "# Step 4: Define the video path and perform inference\n",
        "video_path = '/content/drive/MyDrive/videos/jimmyfallon-fake.mp4'\n",
        "\n",
        "result, fake_count, real_count = detect_deepfake_in_video(model, video_path, transform, device)\n",
        "print(f'The video is: {result}')\n",
        "print(f'Number of fake frames: {fake_count}')\n",
        "print(f'Number of real frames: {real_count}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSQaIP64wKhx",
        "outputId": "360b0d76-9ff0-46a6-9c8b-7075bbbab143"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "'jimmy fallon-fake.mp4'  '#jimmyfallon part 2 😂.mp4'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The video is: Real\n",
            "Number of fake frames: 0\n",
            "Number of real frames: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Replace 'your_video_folder' with the actual folder name where the video is uploaded\n",
        "!ls /content/drive/MyDrive/videos/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUmmNacq0alT",
        "outputId": "9bedb9a0-3ad7-408a-d7c7-ec7dca613202"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "'jimmy fallon-fake.mp4'  '#jimmyfallon part 2 😂.mp4'   saipallavi-fake.mp4   saipallavi.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import albumentations as A\n",
        "\n",
        "# Define the transformation\n",
        "transform = A.Compose([\n",
        "    A.Resize(224, 224),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "# Load the model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = models.resnet50(weights='IMAGENET1K_V1')\n",
        "num_features = model.fc.in_features\n",
        "model.fc = nn.Linear(num_features, 2)  # Assuming binary classification (real or fake)\n",
        "model = model.to(device)\n",
        "\n",
        "# Uncomment if you have saved model weights\n",
        "# model.load_state_dict(torch.load('/path/to/saved_model.pth'))\n",
        "\n",
        "# Step 3: Define the video inference function\n",
        "def detect_deepfake_in_video(model, video_path, transform, device):\n",
        "    model.eval()\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Could not open video.\")\n",
        "        return 'Unknown', 0, 0\n",
        "\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    print(f\"Total number of frames: {frame_count}\")\n",
        "\n",
        "    predictions = []\n",
        "    processed_frames = 0\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        transformed = transform(image=image)\n",
        "        image = transformed['image'].unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(image)\n",
        "            _, pred = torch.max(output, 1)\n",
        "            predictions.append(pred.item())\n",
        "            processed_frames += 1\n",
        "\n",
        "        if processed_frames % 100 == 0:\n",
        "            print(f\"Processed {processed_frames} frames out of {frame_count}\")\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    # Check if any frames were processed\n",
        "    if processed_frames == 0:\n",
        "        print(\"Error: No frames were processed.\")\n",
        "        return 'Unknown', 0, 0\n",
        "\n",
        "    # Determine if the video is mostly fake or real\n",
        "    fake_count = predictions.count(1)\n",
        "    real_count = predictions.count(0)\n",
        "    result = 'Fake' if fake_count > real_count else 'Real'\n",
        "\n",
        "    return result, fake_count, real_count\n",
        "\n",
        "# Step 4: Define the video path and perform inference\n",
        "video_path = '/content/drive/MyDrive/videos/saipallavi-fake.mp4'\n",
        "\n",
        "result, fake_count, real_count = detect_deepfake_in_video(model, video_path, transform, device)\n",
        "print(f'The video is: {result}')\n",
        "print(f'Number of fake frames: {fake_count}')\n",
        "print(f'Number of real frames: {real_count}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2w-zWHO6ZkA",
        "outputId": "84154856-ba1e-436a-d54e-4b4148ec2bfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of frames: 1761\n",
            "Processed 100 frames out of 1761\n",
            "Processed 200 frames out of 1761\n",
            "Processed 300 frames out of 1761\n",
            "Processed 400 frames out of 1761\n",
            "Processed 500 frames out of 1761\n",
            "Processed 600 frames out of 1761\n",
            "Processed 700 frames out of 1761\n",
            "Processed 800 frames out of 1761\n",
            "Processed 900 frames out of 1761\n",
            "Processed 1000 frames out of 1761\n",
            "Processed 1100 frames out of 1761\n",
            "Processed 1200 frames out of 1761\n",
            "Processed 1300 frames out of 1761\n",
            "Processed 1400 frames out of 1761\n",
            "Processed 1500 frames out of 1761\n",
            "Processed 1600 frames out of 1761\n",
            "Processed 1700 frames out of 1761\n",
            "The video is: Fake\n",
            "Number of fake frames: 1761\n",
            "Number of real frames: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AZ405lY-6g-1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}